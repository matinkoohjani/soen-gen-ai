from langchain_core.example_selectors.base import BaseExampleSelector
from langchain_core.prompts.few_shot import FewShotPromptTemplate
from langchain_core.prompts.prompt import PromptTemplate
from gensim.summarization import bm25
import pickle
import openai
import os, csv
from openai import OpenAI

os.environ["OPENAI_API_KEY"] = ""

client = OpenAI()

max_length = 6000
number = 4

def escape_f_string(text):
    return str(text).replace('{', '{{').replace('}', '}}')

class CustomExampleSelector(BaseExampleSelector):
    def __init__(self, examples):
        self.examples = examples

    def add_example(self, example):
        self.examples.append(example)

    def select_examples(self, input_variables):

        query_code = input_variables["input"]

        def simple_tok(sent: str):
            return str(sent).split()
        
        def select_top(samples: list, top_k: int):
            human_top_samples = [x for x in samples if x['output'] == 1][:top_k]
            ai_top_samples = [x for x in samples if x['output'] == 0][:top_k]
            return human_top_samples, ai_top_samples

        tok_corpus = [simple_tok(s["input"]) for s in self.examples]
        bm25_model = bm25.BM25(tok_corpus)
        query = simple_tok(query_code)
        average_idf = sum(map(lambda k: float(bm25_model.idf[k]), bm25_model.idf.keys())) / len(bm25_model.idf.keys())
        scores = bm25_model.get_scores(query, average_idf)

        best_samples = []
        best_codes = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)
        for b in best_codes:
            best_samples.append(self.examples[b])

        human_best_samples, ai_best_samples = select_top(best_samples, 2)

        human_best_samples.reverse()
        ai_best_samples.reverse()

        final_best_samples = human_best_samples + ai_best_samples
        # for sample in final_best_samples:
        #     print(sample['input'], '\n', sample['output'])

        # Replace '{' '}' to '{{' '}}'
        final_best_samples = map(lambda x: {"input": escape_f_string(x['input']), "output": x['output']}, final_best_samples)
        return final_best_samples


file_list = os.listdir('')

from itertools import combinations, permutations, product

def generate_pairs_combinations(lst):
    pairs = list(product(lst, repeat=2))
    return pairs

seq = sorted(generate_pairs_combinations(file_list))

# fill out the examples with your training set
for file_name in os.listdir(''):
    dataset, model_name, lang = file_name.split('_')

    with open(f'', 'rb') as f:
        examples = pickle.load(f)

    with open(f'', 'rb') as f:
        test_examples = pickle.load(f)

    output_csv = open(f'', 'w')
    output_writer = csv.writer(output_csv, delimiter=',', lineterminator='\n', quotechar='"', quoting=csv.QUOTE_MINIMAL)
    output_writer.writerow(['idx', 'code', 'label', 'pred'])


    for i, test_example in enumerate(test_examples):
        example_selector = CustomExampleSelector(examples)

        example_prompt = PromptTemplate.from_template("Input: {input} -> Output: {output}")

        prompt = FewShotPromptTemplate(
            example_selector=example_selector,
            example_prompt=example_prompt,
            suffix="Input: {input} -> Output:",
            prefix=f"You will be provided with a {lang} source code. Please determine if the code snippet is generated by Artificial intelligence (AI) models (i.e. language models) or human. If the code is generated by human, output 1. If the code is generated by AI, output 0. A few example source code along with their corresponding labels are also provided. Return only the label in the output.",
            input_variables=["input"],
        )

        new_query = f"{escape_f_string(str(test_example['input']))}"
        prompt = prompt.format(input=new_query)
        
        completion = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[{"role": "user", "content": prompt}
                ]
        )

        response = completion.choices[0].message.content

        output_writer.writerow([i, test_example['input'], test_example['output'], response])
        print(f"-> #{i} response generated.")

